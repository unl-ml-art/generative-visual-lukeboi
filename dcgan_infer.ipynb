{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e96db35-3dfb-48dc-8390-cf5d69ca927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/emar349/shared/envs/tf-gpu-yourtts/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffed6ee3-c6a1-42d0-bc90-136192ae94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    npimg = npimg-np.amin(npimg)\n",
    "    npimg = npimg/np.amax(npimg)\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24414ea5-951f-41c1-a55e-a1e394e21e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = int(1) # use one GPU\n",
    "nz = int(100) # code dimension (This is the() random noise) input dimension of the generator network)\n",
    "ngf = int(64) # output dimension of the generator network\n",
    "ndf = int(64) # input dim (image size) for the discriminator net\n",
    "nc = 1 # number of input channels (e.g. 3 for RGB channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7db508-9890-472a-860a-69a300681aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create the Generator network\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(_netG, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7806b9c-317e-405c-97d3-81ba3d5b55ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netG(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = _netG(1)\n",
    "netG.load_state_dict(torch.load(\"dcgan_results/netG_epoch_9.pth\"))\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231ac9b0-a4c2-4052-b2cc-8a806e7c112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "def NormalizeData(a):\n",
    "    return (255*(a - np.min(a))/np.ptp(a)).astype(np.uint8)\n",
    "\n",
    "data = []\n",
    "\n",
    "def save_image(fake, name, d):\n",
    "    img = fake.data.cpu().detach().numpy()[0,0]\n",
    "    img = NormalizeData(img)\n",
    "    im = Image.fromarray(img, mode=\"L\")\n",
    "    d = d + list(img.reshape((64 * 64)))\n",
    "    im.save(name)\n",
    "    return d\n",
    "\n",
    "first = torch.FloatTensor(1, nz, 1, 1).normal_(0, 1)\n",
    "second = torch.FloatTensor(1, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "\n",
    "n = 0\n",
    "for i in range(1):\n",
    "    first = second\n",
    "    second = torch.FloatTensor(1, nz, 1, 1).normal_(0, 1)\n",
    "    for j in np.arange(0, 1, 0.1): \n",
    "        fake = netG(torch.lerp(first, second, j))\n",
    "        #show(make_grid(fake.data, padding=5))\n",
    "        data = save_image(fake, \"dcgan_inter/\" + str(n) + \".png\", data)\n",
    "        n += 1\n",
    "        \n",
    "# print(data)\n",
    "data = np.array(data, dtype=np.uint8)\n",
    "print(data.shape[0] / (64 * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2cb1e-c7b7-405f-8791-96d022761513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21733d97-39c0-4491-8c38-2bd06a39fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu-yourtts)",
   "language": "python",
   "name": "tf-gpu-yourtts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
